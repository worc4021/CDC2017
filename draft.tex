\documentclass[letterpaper, 10pt, conference]{ieeeconf} % Change font size to 9pt

\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{graphics}
\usepackage{epsfig}
% \usepackage{mathptmx}
% \usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{cite}
\usepackage{lpic}
\usepackage{overpic}
% \usepackage{blkarray}
\usepackage{mathrsfs}
\usepackage{todonotes}
\usepackage[hidelinks]{hyperref}
\usepackage{caption}

\usepackage{tikz}
\usetikzlibrary{arrows,positioning,patterns,decorations.pathreplacing,calc,shapes.geometric}

\usepackage[backend=biber,
			sortcites=true,
			sorting=none,
			style=numeric-comp,
			firstinits=true,
			doi=false,
			isbn=false,
			url=false
			]{biblatex}
\bibliography{MyBib}


% \DeclareNameFormat{author}{%
%   \ifthenelse{\value{listcount}=1}
%     {\namepartfamily%
%      	\ifblank{\namepartgiven}{}{
% 		\addcomma\space\namepartgiven}}
% 	{\ifblank{\namepartgiven}{}{\namepartgiven\space}%
%    		\namepartfamily}%
% 		\ifthenelse{\value{listcount}<\value{liststop}}
%   	{\addcomma\space}{}
% }

\newcommand*{\Resize}[1]{\resizebox{\columnwidth}{!}{$#1$}}

\newtheorem{thm}{Lemma}[section]
\newtheorem{rem}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assn}{Assumption}
\newtheorem{algo}[thm]{Algorithm}

\providecommand{\norm}[1]{\left\|#1\right\|}
\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\conv}{\text{conv}}
\providecommand{\bfa}[1]{\mathbf{#1}}

\usepackage{parskip}
\setlength{\parskip}{8pt} 
\setlength{\parindent}{0pt}

\usepackage{xcolor}
\def\edit{\textcolor{blue}}
\allowdisplaybreaks[4]


\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10> gen * mathx
      <10.95> mathx10 <12> <14.4> <17.28> <20.74> <24.88> mathx12
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathSymbol{\temp}{\mathbin}{mathx}{'341}
\newcommand{\bigominus}{\raisebox{10pt}{$\temp$}}

\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}

\graphicspath{{./Pix/}}

\begin{document}
\title{A set based approach to chance constrained programming}

\author{Rainer M. Schaich\textsuperscript{\dag} %
         and Mark Cannon\textsuperscript{\dag,\ddag}%
\thanks{\textsuperscript{\dag} Department of Engineering Science, University of Oxford, OX1 3PJ.}%
\thanks{\textsuperscript{\ddag} Corresponding author, 
        \texttt{mark.cannon@eng.ox.ac.uk}.}
}
\newcommand{\note}[1]{\todo[inline]{#1}}

\maketitle

\begin{abstract} 
For a certain class of stochastic programming problems we introduce a method to robustify the problem and thereby reduce the complexity while introducing a minimal amount of uncertainty.
%
An optimal auxiliary polytope is determined using a nonlinear optimisation program, this auxiliary set can then be used to solve a deterministic problem guaranteeing probabilistic constraint satisfaction.
%
We furthermore present a method to determine the probability measure of a polytope efficiently using Riemann-like sums over a grid.
%
Methods to generate both homogeneous and inhomogeneous grids are presented.
%
The proposed scheme is illustrated and compared with state-of-the-art alternatives in an example.
\end{abstract}

\begin{keywords}
keyword 1, keyword 2, keyword 3...
\end{keywords}
%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro}%
%
%
%
%
%
\noindent Optimisation problems with probabilistic constraints on the decision variable arise in various domains of science.
%
A prominent approach to solving such problems is to employ a scenario based approach~\cite{Calafiore:2010}, for which a deterministic optimisation program is solved for a sufficiently large number of sampled probabilistic constraints.
%
This way constraint satisfaction with a chosen confidence is guaranteed, however the number of samples necessary generally depends on the dimension, the desired probability and confidence level and is often unnecessarily large.
%
In cases for which we have more particular knowledge of the meaning of the probabilistic constraints it is often possible to reformulate the stochastic problem as a robust optimisation problem which can be solved using existing methods.
%
The trivial way would be to satisfy the probabilistic constraints with certainty, which reduces the complexity of the problem but increases the conservatism of its solution.
%
Here we present a more sophisticated approach, we determine an auxiliary subset of the uncertain variable for which the probabilistic constraints are satisfied.
%

To be able to determine such auxiliary sets we have to constrain the search space to polytopes of a given combinatorial structure.
%
This allows us to formulate a nonlinear programming problem to determine an optimal auxiliary set which can then be used to robustify the original stochastic program introducing a minimal amount of conservatism.
%
We present a method to approximate the probability measure of the auxiliary set in a computationally tractable way using a grid based discretisation of the probability space.
%


%
This paper is structured as follows:
%
In Section~\ref{sec:problem:formulation} we present the problem description we consider, Section~\ref{sec:optimising:polytopes} discusses the proposed method to reduce the uncountably infinite dimensional optimisation problem introduced in Section~\ref{sec:problem:formulation} to a finite dimensional nonlinear optimisation program.
%
Section~\ref{sec:counting:cubes} deals with the problem of approximating the probability measure of a polytope using a homogeneous discretisation of the probability space, thereby reducing integrals into finite sums which can be efficiently evaluated.
%
We illustrate the proposed scheme in Section~\ref{sec:example} and compare the result with a scenario-based method.
%
A method to determine a inhomogeneous discretisation of the probability space is presented in Section~\ref{sec:improved:grid}.
%
Section~\ref{sec:conclusion} concludes this work and gives possible future directions of research.
%


%
Throughout the paper we use the term \textit{$d$-dimensional polytope} to refer to a bounded set containing a $d$-dimensional ball and formed by the intersection of a finite number of halfspaces or equivalently by the convex hull of a finite number of points.
A \textit{face} of a polytope is its intersection with a hyperplane such that the polytope lies on one side of the hyperplane; the dimension of the face is the dimension of the affine hull of this intersection; one-, two- and ${(d-1)}$-dimensional faces are called \textit{vertices}, \textit{edges} and \textit{facets} respectively. 
We use $\mathcal Q \cong \mathcal S$ to indicate that polytopes~$\mathcal Q$ and $\mathcal S$ are \textit{combinatorially equivalent}.
%
The sets of real numbers and non-negative reals are denoted $\mathbb R$ and $\mathbb R_+$, and~$\bfa{1}$ is a vector of ones. 
%
For sets $\mathcal X,\mathcal Y\subseteq\mathbb R^d$, the Minkowski sum is denoted $\mathcal X \oplus \mathcal Y = \{ x + y : x\in\mathcal X, \ y\in \mathcal Y\}$, the Pontryagin difference is $\mathcal X \ominus \mathcal Y = \{ x: \forall y \in \mathcal Y, \ x+ y\in\mathcal X \}$, the image of $\mathcal X$ under a map $g:\mathbb R^d \to \mathbb R^q$ is denoted $g(\mathcal X) = \{z : \exists\, x\in\mathcal X, \ z = g(x)\}$, and $\mathscr P (\mathcal X)$ denotes the power set consisting of all subsets of $\mathcal X$.

% Two polytopes~$\mathcal Q,\mathcal S$ are combinatorially equivalent if there exists a map between them which preserves the inclusion, i.e. if $q_i$ is a vertex of~$\mathcal Q$ and is contained in the edge~$e_j$ then the corresponding vertex~$s_i$ is contained in the corresponding edge~$f_j$ etc.
%
\section{Problem Formulation}\label{sec:problem:formulation}
%
%
%
We consider an optimisation problem in which the decision variable is a subset of a given set $\Omega \subseteq \mathbb R^d$: 
%
\begin{subequations}\label{seq:main:problem}
\begin{equation}
\maximize_{\mathcal V\subseteq \Omega} \ f(\mathcal V)
\end{equation}
subject to
\begin{align}
\label{eq:probabilistic:constraint:set:formulation}
        & \Pr (v\in\mathcal V) \geq p \\
\label{eq:inclusion:constraint:set:formulation}
        & g(v)\in\mathcal Y \text{ for all } v \in \mathcal V.
\end{align}
\end{subequations}
%
Here $f: \mathscr P (\Omega) \to \mathbb R_+$, $g:\mathbb R^d\rightarrow \mathbb R^q$, $\mathcal Y\subset\mathbb R^q$ is a given set, $\Pr(\cdot)$ denotes the probability of event $(\cdot)$  and $p\in (0,1]$ is a specified probability.
%
For clarity we can think of~$f$ as the volume (or more generally the measure) of a set that depends on $\mathcal V$ (see for example Section~\ref{sec:example}, where $f(\mathcal V) = \text{vol}(\mathcal X\ominus\mathcal V)$ for a
given set~$\mathcal X$).
%where $\ominus$ denotes the Pontryagin set difference, i.e.~$\mathcal X\ominus \mathcal V = \{x: x + v \in \mathcal X \text{ for all } v\in\mathcal V\}$.
%

The relevant probability space in~\eqref{eq:probabilistic:constraint:set:formulation} (for which we assume the density function is known) is~$(\Omega, \mathscr P(\Omega), \mathbb P)$.
%, and we assume that the density function is known
Hence the constraints~\eqref{eq:probabilistic:constraint:set:formulation} and \eqref{eq:inclusion:constraint:set:formulation}
 are equivalent to
%
\begin{subequations}
\begin{align}
	& \mathbb P\{\mathcal V\}\geq p \\
        & g(\mathcal V) \subseteq \mathcal Y .
\end{align}
%
% \begin{equation}
% 	g(\mathcal V)\subseteq\mathcal Y.
% \end{equation}
\end{subequations}
%
A key problem is therefore to determine the probability measure of the set variable~$\mathcal V$.
%
In this work we consider problems of the form~\eqref{seq:main:problem} in which the decision variable~$\mathcal V$ is constrained to be polytopic. In particular we impose the constraint that the auxiliary set $\mathcal V$ has a specific \emph{combinatorial structure}, i.e.~$\mathcal V\cong\mathcal V_0$ for a given set $\mathcal V_0$.
%

The focus of the remainder of this paper is on constructing a method of approximating the solution of~\eqref{seq:main:problem} using conventional nonlinear programming methods.

\section{Optimising polytopes with fixed combinatorial structure}\label{sec:optimising:polytopes}
%
%
%
\noindent In order to derive an optimisation method for polytopes we recall the following definition:
%
\begin{defn}[\cite{Ziegler:1995}]
The polytopes~$\mathcal Q$ and~$\mathcal S$ are combinatorially equivalent if there exists a map~$\mathcal T$ that maps~$\mathcal Q$ onto~$\mathcal S$ while preserving faces, i.e. vertices of~$\mathcal Q$ map to vertices of~$\mathcal T(\mathcal Q)=\mathcal S$, edges of~$\mathcal Q$ map to edges of~$\mathcal S$ and so on.
\end{defn}
%
Given the polytope~$\mathcal V = \{x:a_ix\leq b_i,1\leq i\leq M\}=\text{conv}_{1\leq j\leq N}\{v_j\}$, we define the index set~$\mathcal A_j\subset\{1,\dots,M\}$ by
%
\begin{equation}\label{eq:definition:of:index:set}
	\mathcal A_j = \bigcup_{\substack{1\leq i\leq M\\ a_i v_j=b_i}}\{i\}.
\end{equation}
%
Notice that knowledge of~$\mathcal A_j$ for all~$j\in\{1,\dots,N\}$ uniquely determines the entire combinatorial structure of~$\mathcal V$ since all $n$-dimensional faces are such that $\abs{\mathcal A_{j_1}\cap\mathcal A_{j_2}}=d-n+1$.
%
We therefore have the following statement:
%
\begin{thm}\label{thm:combinatorial:equivalence}
Let~$\mathcal Q$ and $\mathcal S$ be full dimensional polytopes in~$\mathbb R^d$, then $\mathcal Q\cong\mathcal S$ if and only if they induce the same index sets~$\mathcal A_j$ as defined in~\eqref{eq:definition:of:index:set}.
\end{thm}
%
Lemma~\ref{thm:combinatorial:equivalence} provides a canonical way to rewrite the optimisation problem~\eqref{seq:main:problem} for combinatorially constrained~$\mathcal V\cong\mathcal V_0$:
%
\begin{subequations}\label{seq:simplified:problem:description}
\begin{equation}
	\minimize_{a_i,b_i,v_j} f(\mathcal V) % \min_{\substack{a_1,b_1\\a_2,b_2\\ \vdots \\ v_1\\v_2\\ \vdots}}f(\mathcal V)
\end{equation}
%
subject to
%
\begin{alignat}{2}
        & a_i v_j=b_i, & & i\in\mathcal A_j\\
	& a_i v_j\leq b_i-\epsilon, & & i\not\in\mathcal A_j \\
%
	& v_j\in\Omega,  & & j\in\{1,\dots,N\} \\
%
	& \mathcal V = \conv\{v_j\}_{j\in\{1,\dots,N\}} \\
%
\label{eq:probability:of:decision:variable}
	& \mathbb P\{\mathcal V\}\geq p \\
%
\label{eq:nonlinear:inclusion}
	& g(\mathcal V)\subseteq\mathcal Y
\end{alignat}
\end{subequations}
%
where~$\epsilon>0$ denotes a tolerance, which can be used as a design variable.
%
For general problems of the form~\eqref{seq:simplified:problem:description}, both~\eqref{eq:probability:of:decision:variable} and~\eqref{eq:nonlinear:inclusion} pose computational challenges.
%
Constraints of the type~\eqref{eq:nonlinear:inclusion} arise in min-max control problems, where~$g(\mathcal V)$ becomes an affine transformation of the set~$\mathcal V$ (see e.g.~\cite{Schaich:2017}), i.e.~$g(\mathcal V) = C\mathcal V\oplus \mathcal S$ for some known matrix~$C\in\mathbb R^{q\times d}$ and a known set~$\mathcal S$. 
%
Such affine maps can be explicitly inverted using the Pontryagin difference and we therefore omit a general treatment of~\eqref{eq:nonlinear:inclusion}.
%
\begin{rem}
The constraints introduced in~\eqref{seq:simplified:problem:description} generally result in a non-convex feasible set.
%
We note, however, that the question of convexity of the general problem~\eqref{seq:main:problem} is ill-posed because the set of measurable sets (on which~\eqref{seq:main:problem} could be sensibly defined) is not a vector space, and in fact even the set of convex measurable sets is not a vector space.
In particular, $f(\lambda\mathcal V_1\oplus(1-\lambda)\mathcal V_2)\leq\lambda f(\mathcal V_1)+(1-\lambda)f(\mathcal V_2)$ for all $\lambda\in[0,1]$ does NOT imply that there exists a unique optimum~$\mathcal V^\prime$ such that $f(\mathcal V^\prime)\leq f(\mathcal V^\prime\oplus\tilde{\mathcal V})$ for all admissible~$\tilde{\mathcal V}$.
%
%Since the property of convexity relies on certain structures of the topology on which it is defined, no useful definition of convexity for problem~\eqref{seq:main:problem} can be made, therefore we cannot expect a simplification such as~\eqref{seq:simplified:problem:description} to be a convex finite dimensional problem.
%\textcolor{red}{I'm not sure if this is a particularly good remark.. I think it is important to say that something like~$f(\lambda\mathcal V_1\oplus(1-\lambda)\mathcal V_2)\leq\lambda f(\mathcal V_1)+(1-\lambda)f(\mathcal V_2)$ does NOT imply that there exists a unique optimum~$\mathcal V^\prime$ such that $f(\mathcal V^\prime)\leq f(\mathcal V^\prime\oplus\tilde{\mathcal V})$ for all admissible~$\tilde{\mathcal V}$.}
\end{rem}
%
\noindent Constraint~\eqref{eq:probability:of:decision:variable} is a constraint on the probability measure of~$\mathcal V$:
%
\begin{equation}\label{eq:integral:probability}
	\mathbb P\{\mathcal V\} = \int_{\mathcal V}\mathfrak f(x) \; \mathrm d x
\end{equation}
%
where~$\mathfrak f(x)$ denotes the probability density function of the random variable in the problem definition~\eqref{seq:main:problem}.
%
In the next section we discuss a method to approximate integrals like~\eqref{eq:integral:probability}.

\section{Finite approximations of probability measures}\label{sec:counting:cubes}
%
%
%
%
\noindent To be able to solve the optimisation program~\eqref{seq:simplified:problem:description} with existing nonlinear programming solvers we require a numerically tractable way of evaluating the probabilistic measure of the decision variable~\eqref{eq:probability:of:decision:variable}, i.e. a way of determining the integral~\eqref{eq:integral:probability}.
%
To this end we define the homogeneous grid~$Z$ over $\Omega\subseteq\mathbb R^d$ using cubes, denoted $Q_{\bfa{i}}$, defined in terms of a multi-index $\bfa{i}=\{i_1,\ldots,i_d\}$ by
%
\begin{equation}
	Q_{\bfa{i}} =\left\{x\in\mathbb R^d: \delta i_j \leq x_j\leq\delta (i_j+1)\;\forall j\in\{1,\dots,d\} \right\} .
\end{equation}
%
The definition of the grid, which clearly depends on the choice of~$\delta$, is thus
%
\begin{equation}
	Z = \bigcup_{\substack{\bfa{i}\in\mathbb Z^d\\ Q_\bfa{i}\cap\Omega\neq\emptyset}} Q_\bfa{i} .
\end{equation}
%
Defining
%
\begin{equation}
	\mathfrak f_\bfa{i} = \int_{Q_\bfa{i}}\mathfrak f(x)\; \mathrm d x ,
\end{equation}
%
we can approximate the integral~\eqref{eq:integral:probability} using the finite sums
%
\begin{align}
\underline{P}(\mathcal V) &= \sum_{Q_\bfa{i}\subseteq\mathcal V}\mathfrak f_\bfa{i}\label{eq:inner:approx}\\
\overline{P}(\mathcal V) &= \sum_{Q_\bfa{i}\cap\mathcal V\neq\emptyset}\mathfrak f_\bfa{i} \label{eq:outer:approx}\\
\mathring{P}(\mathcal V) &= \sum_{c_\bfa{i}\in\mathcal V}\mathfrak f_\bfa{i}\label{eq:center:approx}
\end{align}
%
where~$c_\bfa{i}=(\bfa{i}+\frac{1}{2}\bfa{1})\delta$ denotes the centre of the cube~$Q_\bfa{i}$.
%
These sums become meaningful with the following statement.
%
\begin{thm}\label{thm:sandwich:inequality}
The sums~$\underline P(\mathcal V)$,~$\overline{P}(\mathcal V)$ and~$\mathring{P}(\mathcal V)$ satisfy
%
\begin{align}
\underline{P}(\mathcal V)\leq \mathring{P}(\mathcal V)\leq\overline{P}(\mathcal V)\label{eq:sandwich:synthetical}\\
\underline{P}(\mathcal V)\leq \mathbb P\{\mathcal V\}\leq\overline{P}(\mathcal V)\label{eq:sandwich:analytical}
\end{align}
%
furthermore $\lim_{\delta\searrow0}\underline{P}(\mathcal V)=\mathbb{P}(\mathcal V)=\lim_{\delta\searrow0}\overline{P}(\mathcal V)$.
\end{thm}
%
\begin{proof}
Identity~\eqref{eq:sandwich:synthetical} follows trivially from the fact that~$c_\bfa{i}\in\mathcal V$ implies $Q_\bfa{i}\cap\mathcal V\neq\emptyset$ and $Q_\bfa{i}\subseteq\mathcal V$ implies $c_\bfa{i}\in\mathcal V$.
%
For identity~\eqref{eq:sandwich:analytical} we use the fact that~$\underline P(\mathcal V)$ and~$\overline{P}(\mathcal V)$ are respectively the exact integrals (in the sense of the Lebesgue measure) over inner and outer approximations of~$\mathcal V$, and~\eqref{eq:sandwich:analytical} and~$\lim_{\delta\searrow0}\underline{P}(\mathcal V)=\mathbb{P}(\mathcal V)=\lim_{\delta\searrow0}\overline{P}(\mathcal V)$ therefore follow.
\end{proof}
%
For general polytopes~$\mathcal V\subseteq\mathbb R^d$ evaluating~$\underline{P}(\mathcal V)$ requires checking, for each $Q_{\bfa i} \subseteq Z$, whether the cube $Q_{\bfa i}$ is a subset of $\mathcal{V}$, where the number of multi-indices~$\bfa{i}\in\mathbb Z^d$ with~$Q_\bfa{i}\cap\mathcal V\neq\emptyset$ can be overestimated by~$D_1D_2\dots D_d \delta^{-d}$.
%
Where $D_i = \{\min_{\overline{x_i},\underline{x_i}} \overline{x_i}-\underline{x_i} : \underline{x_i}\leq x_i\leq\overline{x_i}\; \forall x\in\Omega\}$.
%
Checking whether~$Q_\bfa{i}\subseteq\mathcal V$ requires the solution of a linear program with~$2dM$ decision variables (see e.g.~\cite{Schaich:thesis}), and for larger numbers of multi-indices the computation therefore becomes intractable.
%
Similarly, checking whether~$\mathcal V\cap Q_\bfa{i}\neq\emptyset$ requires the solution of a linear program with~$d$ decision variables, again becoming intractable for large grids.
%
Recall that nonlinear programming solvers would not compute~$\underline{P}(\mathcal V)$ and~$\overline{P}(\mathcal V)$ once but repeatedly throughout the optimisation.
%


%
Although the relation between~$\mathbb P\{\mathcal V\}$ and~$\mathring{P}(\mathcal V)$ is not known, from Lemma~\ref{thm:sandwich:inequality} we have~$\lvert\mathbb P\{\mathcal V\}-\mathring{P}(\mathcal V)\rvert \leq \overline{P}(\mathcal V)-\underline{P}(\mathcal V)$, and therefore
\[
\mathring{P}(\mathcal V)\xrightarrow{\delta\searrow0}\mathbb{P}(\mathcal V).
\]
%
Determining whether a point $c_\bfa{i}$ is contained in a polytope~$\mathcal V$ does not require linear programming methods but simply that $a_k c_\bfa{i}\leq b_k$ holds for all~$k\in\{1,\dots,M\}$.
%
To do this efficiently let
\[
\sigma(t)=\begin{cases} 1, & t\geq0 \\
0, & t<0
\end{cases}
\]
denote the Heaviside function and let~$\gamma(t) = \sigma(-t)$, furthermore let
%
\begin{equation}
	g_\bfa{i} = \sum_{k=1}^M\gamma(a_k c_\bfa{i}-b_k).
\end{equation}
%
Clearly, $c_\bfa{i}\in\mathcal V$ is equivalent to~$g_\bfa{i}=M$, or~$g_\bfa{i}-M+\rho\geq0$ for any~$0\leq\rho$, and with this we have the explicit formula
%
\begin{equation}\label{eq:exact:formula:mathring}
	\mathring{P}(\mathcal V) = \sum_{\bfa{i}}\sigma(g_\bfa{i}-M+\rho)\mathfrak f_\bfa{i}.
\end{equation}
%
Since the discontinuity of the Heaviside function~$\sigma(t)$ can cause problems for nonlinear programming solvers, we therefore relax~\eqref{eq:exact:formula:mathring} using a smooth approximation.
%


%
To construct a smooth approximation of the Heaviside function~$\sigma(t)$,
%and hence~\eqref{eq:exact:formula:mathring} smoothly 
we propose the use of the logistic Sigmoid function~$S(\alpha,t) = (1-e^{-\alpha t})^{-1}$, which has various properties that make it favourable for our purposes.
%
In particular the derivative property~$\frac{\mathrm d S}{\mathrm d t}(\alpha,t)=S(\alpha,t)(1-S(\alpha,t))\alpha$ allows us to obtain the derivative of the approximation
%
\begin{equation}\label{eq:central:approximation:formula}
\begin{aligned}
\tilde{P}(\mathcal V) &= \sum_\bfa{i}S(\alpha,\tilde g_\bfa{i}-M+\rho)\mathfrak f_\bfa{i}\\
\tilde g_\bfa{i} &= \sum_{k=1}^M S(\alpha,b_k-a_k c_\bfa{i}+\tilde\rho)
\end{aligned}
\end{equation}
%
with a minimal number of function evaluations, where~$\tilde\rho>0$ is an appropriately chosen tolerance.
%
It is worth pointing out that the logistic function converges point-wise towards the Heaviside function~$\abs{S(\alpha,t)-\sigma(t)}\xrightarrow{\alpha\rightarrow\infty}0$ for all~$t\in\mathbb R\setminus\{0\}$ and~$S(\alpha,0)=\frac{1}{2}$ for all~$\alpha\in\mathbb R$.
%
This allows us to specify criteria to design appropriate~$\rho>0$; for this let~$\Delta(\alpha,\eta)=t_2-t_1$ denote the length of the interval such that~$S(\alpha,t_1)=\eta$ and~$S(\alpha,t_2)=1-\eta$ for~$0<\eta<1$.
%
Using simple arithmetic we find~$\Delta(\alpha,\eta)=\frac{2}{\alpha}(\log(1-\eta)-\log(\eta))$. In order to choose~$\rho$ and~$\alpha$ so as to suppress ambiguities between~$c_\bfa{i}$ and its neighbours~$c_\bfa{j}$, a sensible choice would be to make~$\Delta(\alpha,\eta)<\delta$, so that $\Delta(\alpha,\eta)$ is smaller than the minimal distance between two centre points~$c_\bfa{i}$ and~$c_\bfa{j}$. 
%
The canonical choice for~$\tilde\rho$ is therefore given by~$\tilde\rho=\frac{\Delta(\alpha,\eta)}{2}$ which produces~$S(\alpha,\tilde\rho)=1-\eta$.
%
Assume~$c_\bfa{i}$ to be a vertex of~$\mathcal V$, i.e.~$\tilde g_\bfa{i}>(1-\eta)d+(1-\eta)(M-d)$ where the first term is exact and the~$(1-\eta)(M-d)$ term can be bounded approximately as~$M-d$.
%
In particular, notice that it is not possible for a point~$c_\bfa{i}\in\mathcal V$ to have a value~$g_\bfa{i}=(1-\delta)M$ since this implies that $c_\bfa{i}$ lies on all~$M>d$ hyperplanes.
%
Therefore, $f_\bfa{i}$ contributes to~$\tilde{P}(\mathcal V)$ if~$\tilde g_\bfa{i}\geq(1-\eta)d+(M-d)-\rho$ where $\rho$ can be chosen similarly to~$\tilde\rho$.
%
These choices for~$\rho$ and~$\tilde\rho$ yield
%
\begin{equation}\label{eq:sandwich:from:sigmoid}
	\tilde P(\mathcal V)\leq\mathring P(\mathcal V)\leq \frac{\tilde P(\mathcal V)}{1-\eta}.
\end{equation}
%
The left hand side of~\eqref{eq:sandwich:from:sigmoid} follows from the way that~$\rho$ and~$\tilde\rho$ were chosen to ensure that~$S(\alpha,t+\rho)<\sigma(t)$ for all~$t\geq0$ and~$S(\alpha,t+\rho)\leq\eta$ for all~$t\leq -\Delta(\alpha,\eta)$, leaving only the interval~$[-\Delta(\alpha,\eta),0]$ for aliasing.
%
However, since we concatenate the two Sigmoid functions we obtain that the~$\tilde g_\bfa{i}>M-1$ contribute to~$\tilde P(\mathcal V)$ and this strict inequality cannot be satisfied unless~$c_\bfa{i}\in\mathcal V$ is in all~$M$ halfspaces. Therefore the interval~$[-\Delta(\alpha,\eta),0]$ does not contribute to the result as long as~$\Delta(\alpha,\eta)<1$, which can easily be satisfied by increasing~$\alpha>0$.
%
We illustrate the proposed method in a numerical example in the following section.

\section{Example}\label{sec:example}
%
%
\noindent In order to illustrate the proposed method we use an example motivated by a robust model predictive control approach to a stochastic problem formulation.
%
Consider the problem of finding the largest set of states~$\mathcal X$ for the system~$x^+=x+v$ (where $x^+$ denotes the successor state) such that
\[
\Pr (x^+\in\mathcal Y) \geq \frac{3}{10}
\]
holds for~$\mathcal Y = \{y\in\mathbb R^2:-5\leq y_i\leq 4\}$, where the random variable~$v\in\Omega=\{\omega\in\mathbb R^2:\abs{\omega_i}\leq2\}$ has a known probability density function~$\mathfrak f(v)$.
%
The density~$\mathfrak f(v)$ we consider is a truncated normal distribution defined by
\[
\mathfrak f(v) = \frac{1}{c}\exp(-v_1^2-2v_2^2)
\quad \text{with} \quad
c=\frac{\pi\,\text{erf}(2)\,\text{erf}(2\sqrt{2})}{\sqrt 2}
\]
(so that~$\mathbb P\{\Omega\}=1$).
%
Naturally this problem can be formulated as
%
\begin{subequations}\label{seq:example:original}
\begin{equation}
	\maximize_{\mathcal V \subseteq\Omega} \ \text{vol}(\mathcal X)
\end{equation}
subject to
\begin{align}
	&\mathbb P\{\mathcal V\}\geq \frac{1}{3} \\
%\end{equation}
% \begin{equation}
% 	\mathcal V\subseteq\Omega
% \end{equation}
%\begin{equation}
	&\mathcal X = \mathcal Y\ominus \mathcal V
\end{align}
\end{subequations}
%
where the Pontryagin difference in (\ref{seq:example:original}c) guarantees satisfaction of~$x^+\in\mathcal Y$ for all~$v\in\mathcal V$.
%
To apply the proposed method we choose an initialising polytope~$\mathcal V_0$ which defines the combinatorial structure of~$\mathcal V$, for this we use 
%
\[
\mathcal V_0 = \conv\left\{\begin{pmatrix}\pm 1\\0 \end{pmatrix},\begin{pmatrix}0\\\pm1\end{pmatrix},\begin{pmatrix}\pm\frac{3}{4}\\\pm\frac{3}{4}\end{pmatrix}\right\}
\]
%
and the index sets~$\mathcal A_i$ are given as~$\mathcal A_i=\{i,i+1\}$ for $i\in\{1,\dots,5\}$ and~$\mathcal A_6=\{1,6\}$ (the numerical values depend on the half-space description employed but may be rearranged).
%


%
Before rewriting~\eqref{seq:example:original} using~$\mathring P(\mathcal V)$, we first define a grid and evaluate~$\mathfrak f_\bfa{i}$.
%
We choose~$\delta=\frac{1}{100}$, this yields a total of~$4^2 100^2=160000$ cubes~$Q_\bfa{i}$ making up the grid~$Z$.
%
To deal with large numbers of double integrals over cubes~$Q_\bfa{i}$ efficiently we propose the following method.
%
Since
\begin{align*}
\mathfrak f_\bfa{i}=\int_{Q_\bfa{i}}\mathfrak f(v) \; \mathrm d v = &\int_{\delta i_1}^{\delta(i_1+1)}\int_{\delta i_2}^{\delta(i_2+1)}\mathfrak f(v_1,v_2) \; \mathrm d v_2 \; \mathrm d v_1 \\
= &\int_0^\delta\int_0^\delta \mathfrak f(v_1+\delta i_1,v_2+\delta i_2) \; \mathrm d v_2 \; \mathrm d v_1 ,
\end{align*}
we can evaluate the inner integral for all relevant~$\bfa{i}$ simultaneously by introducing
%
\begin{multline*}
\mathfrak F(v_1) =
\int\limits_0^\delta 
\bigl[ 
\mathfrak f(v_1-200\delta,v_2-200\delta) , \mathfrak f(v_1-200\delta,v_2-199\delta), \\
\ldots, \mathfrak f(v_1+199\delta,v_2+199\delta) \bigr] \; \mathrm d v_2 .
\end{multline*}
%
The values for all $\mathfrak f_{-200,-200}$ to~$\mathfrak f_{199,199}$ are then obtained by integrating with respect to~$v_1$, i.e. 
\[
\bigl[ \mathfrak f_{-200,-200}, \mathfrak f_{-200,-199},\dots,\mathfrak f_{199,199} \bigr] =\int_0^\delta\mathfrak F(v_1) \; \mathrm d v_1.
\]
%
This approach can save a significant amount of time in comparison to evaluating each double integral individually and can easily be extended to higher dimensions.
%
Next we choose the threshold~$\eta=\frac{1}{10}$, i.e.~$\mathfrak f_\bfa{i}$ contributes to~$\tilde{P}$ if~$\tilde g_\bfa{i}\approx5.8$ or greater.
%
To avoid aliasing we choose~$\alpha=5000$, this yields~$\Delta(5000,10^{-1}) = \frac{2\log9}{5000}\approx0.0008789<\delta$,
%


%
To evaluate the performance of the proposed method we compare it with a scenario-based approach similar to the one described in~\cite{Margellos:2014}.
%
For this we draw a sufficient number of samples~$\{v_1,\dots,v_n\}\subseteq\Omega$ of the random variable, where the number~$n$ is chosen to satisfy
\[
\Phi(1-p,d,n) = \sum_{k=0}^{d}\binom{n}{k}(1-p)^kp^{n-k}<\beta
\]
for a confidence~$\beta>0$ (see e.g.~\cite{Calafiore:2010}), which we choose as~$\beta=\frac{1}{1000}$.
%
The necessary number~$n$ can be upper bounded by the smallest integer satisfying~$\tilde n>\frac{2}{1-p}(d-\log(\beta))$, however for this example we can explicitly evaluate~$\Phi(\frac{7}{10},2,n)$ to find $\Phi(\frac{7}{10},2,\{10,11\}) = \{0.00159039,0.000577696\}$, i.e. eleven samples are required for scenario based statements to hold with confidence larger then~$1-\beta=\frac{999}{1000}$.
%
Notice that for higher dimensional random variables~$\Phi(1-p,d,n)$ cannot be numerically determined to sufficient accuracy and the upper bound has to be used, which in this case is given by~$\tilde n=26$.
%
In order to obtain a set from the~$n$ samples we use the convex hull~$\hat{\mathcal V}=\conv\{v_i\}$ to compare with the result of the optimisation~\eqref{seq:example:original}.
%
To allow a better comparison we draw $500$~samples and pick~$100$ collections of~$11$ points at random to compute convex hulls for the comparison.
%
The resulting sets are illustrated in Figure~\ref{fig:example:in:comparison}, and we refer to Table~\ref{tab:only:table} for numerical values, notice that we can evaluate both~$\underline{P}(\mathcal V)$ and~$\overline{P}(\mathcal V)$ for a given set~$\mathcal V$, this takes~$174$ and~$143$ seconds respectively in comparison to the evaluation of~$\mathring{P}(\mathcal V)$ which takes~$0.0864$ seconds (where all computations are implemented in Matlab and timed on a 2.3 GHz MacBook Pro, all linear programs are solved using Gurobi~7).
%
Due to issues related to the nonlinear programming solver, the objective function used to obtain the result shown in Figure~\ref{fig:example:in:comparison} is~$\log(1+\text{vol}(\mathcal X))$ rather than~$\text{vol}(\mathcal X)$.
%


%
Notice that the error bound~$e(\mathcal V) = \lvert\mathbb P\{\mathcal V\}-\mathring{P}(\mathcal V)\rvert$ depends on the measure of the cubes which intersect the boundary of~$\mathcal V$, this means that for non-uniform distributions the error~$e(\mathcal V)$ decreases with the distance from the expected value.
%
This implies that a non-uniform grid size such that for each $Q_\bfa{i}$ the measure~$0<\mathfrak{f}_\bfa{i}<\epsilon$ for some fixed~$\epsilon$ may be used to reduce the number of necessary cubes as well as the dependence of the error~$e(\mathcal V)$ on the inner radius of~$\mathcal V$.

\begin{figure}\centering
\include{result.tikz}
\captionsetup{font=small}
\caption{Outlined in black the result~$\mathcal V$ of the optimisation~\eqref{seq:example:original}, the \textcolor{red}{red points} show samples drawn for this distribution and $100$~red outlined sets are convex hulls of a random selection of~$N=11$ samples such that any~$\textcolor{red}{\hat{\mathcal V}}$ allows statements on~\eqref{seq:example:original} to be made with a confidence larger than~$0.999$.
%
The result for the non-homogeneous grid discussed in Section~\ref{sec:improved:grid} is outlined in~\textcolor{blue}{blue}.}
\label{fig:example:in:comparison}
\end{figure}
%
%
%
\section{A Non-Homogeneous Grid}\label{sec:improved:grid}
%
%
%
\noindent In the previous section we presented a simple example using a homogeneous grid, this led to a large number of cubes~$Q_\bfa{i}$ and hence centre points~$c_\bfa{i}$.
%
Since the central approximation~\eqref{eq:central:approximation:formula} does not require a homogeneous grid and neither does Lemma~\ref{thm:sandwich:inequality}, we present in this section a method to determine a grid which reflects the properties of the probability density function more closely.
%
We consider a compact sample space~$\Omega\subseteq\mathcal B$ for some box~$\mathcal B=\{x\in\mathbb R^d:-\mathfrak{c}\leq x_i\leq \mathfrak{c}\}$.
%
The algorithm we suggest is to define a polytopal complex~$\mathcal C$ consisting of cubes~$Q_i$ such that~$\bigcup_{Q_i\in\mathcal C}Q_i=\mathcal B$ and the associated probability measure of each cube is below a certain threshold~$\mathfrak{f}_i\leq\mathfrak{f}_{\max}$.
%
To do this efficiently first notice that
\begin{multline*}
\int_0^{\delta}\dots\int_0^\delta \mathfrak{f}(t_1,\dots,t_d)\; \mathrm d t_1\dots \mathrm dt _d = \\ \delta^d\int_0^1\dots\int_0^1 \mathfrak {f}(\delta\tau_1,\dots,\delta\tau_d)
\; \mathrm d\tau_1\dots \mathrm d\tau_d .
\end{multline*}
Although this is a trivial substitution, it allows the simultaneous computation of~$\mathfrak{f}_i$ over differently sized cubes~$Q_i$.
%
Let~$\{q_1,\dots,q_{2^d}\}$ denote the vertices of the cube~$\mathcal Q=\{x\in\mathbb R^d: 0\leq x_i\leq 1\}$. For this algorithm it is convenient to characterise cubes by their \emph{lower left corner}~$b_i$ and their side length~$\delta_i$, so that the $i$th cube is given by~$Q_i=\conv_{1\leq k\leq2^d}\{b_i+\delta_i q_k\}$, and the associated measure is given by
\[
\mathfrak{f}_i=\int_{\mathcal Q}\mathfrak{f}(b_i+\delta_i\bfa{1} t) \; \mathrm d t.
\]
%
We propose the following algorithm to determine~$\mathcal C$:
%
\begin{algo}\label{algo:non:homo:cubes}
Choose desired~$\mathfrak{f}_{\max}>0$ and~$\delta_{\min}>0$, set~$\mathcal C=\mathcal B$ and evaluate~$\mathfrak{f}_i$\\
\textbf{WHILE} ($\max_i\{\mathfrak{f}_i\}\geq\mathfrak{f}_{\max}$~\textbf{AND}~$\min\{\delta_i\}\geq\delta_{\min}$) \textbf{DO} $\bigl($\\
$IDX := \mathfrak{f}_i\geq \mathfrak{f}_{\max}$\\
\textbf{FOR} $i\in IDX$ $\{$ \textbf{REPLACE} $b_i$ by $b_i+q_k$, $k\in\{1,\dots,2^d\}$ 
and $\delta_i$ by~$\frac{\delta_i}{2}\}$\\
\textbf{EVALUATE}~$\mathfrak f_i, i\in IDX\\
\textbf{UPDATE}~\mathcal C$
$\bigr)$
\end{algo}
%
Algorithm~\ref{algo:non:homo:cubes} terminates either when every cube contains a probability measure below the threshold $\mathfrak{f}_{\max}$ or when some cubes become smaller than the threshold $\delta_{\min}$. Otherwise it replaces all cubes for which the probability measure is larger than~$\mathfrak{f}_{\max}$ with cubes with a halved side length.
%
Once the grid is found the centre points are given by~$c_i=b_i+\frac{\delta_i}{2}\bfa{1}$.
%


%
Algorithm~\ref{algo:non:homo:cubes} can reduce the computational overhead significantly:
%
For the same example as in Section~\ref{sec:example} with a non-homogeneous grid satisfying~$\mathfrak{f}_i\leq\frac{1}{20000}$ which yields~$\min_i\{\delta_i\}=\frac{1}{2^7}=0.0078125$,~$\max_i\{\delta_i\}=\frac{1}{2}$ and a total of~$45436$ cubes.
%
Since~\eqref{eq:central:approximation:formula} does not depend on the choice of the grid itself the remaining optimisation is unchanged, a single evaluation of~$\mathring{P}(\mathcal V)$ now takes $0.0251$ seconds,
%~$0.025115$, 
i.e. about five times  faster than the homogeneous counterpart. The resulting set~$\mathcal V$ is shown in \textcolor{blue}{blue} in Figure~\ref{fig:example:in:comparison},
% and numerical values are reported in Table~\ref{tab:only:table}.
%
For illustrative purposes, Figure~\ref{fig:inhomogeneous:grid} shows a (slightly less fine) grid constructed using Algorithm~\ref{algo:non:homo:cubes}.

%
We summarise the numerical values of the proposed schemes in Table~\ref{tab:only:table}.
%
The first row of the table shows values for the homogeneous grid with~$160000$ cubes (with no direct influence on the measure of each box), the second row gives values for the inhomogeneous grid, while the third row shows the values obtained with the grid shown in Figure~\ref{fig:inhomogeneous:grid}. 
%
Both scenario based sets are evaluated on the inhomogeneous grid, with~$\underline{P}(\mathcal V)$ and~$\overline{P}(\mathcal V)$ evaluated at extreme values of the generated convex hulls, and the values of $\mathring{P}(\mathcal V)$ are the  arithmetic means of all $100$ sample sets.
%

%
%
%
%
%
\begin{figure}\centering
\ifodd0
\include{grid.tikz}
\else
\vspace{88mm}
\fi
\captionsetup{font=small}
\caption{The inhomogeneous grid generated by Algorithm~\ref{algo:non:homo:cubes} for the values $\max\{\mathfrak{f}_i\}\leq\frac{1}{10000}$ and~$\min\{\delta_i\}\geq\frac{1}{10000}$ which produces~$20404$ cubes.
%
Outlined in \textcolor{blue}{blue} the optimised set~$\mathcal V$.}
\label{fig:inhomogeneous:grid}
% \vspace{-2em}
\end{figure}
%
%
\begin{table}
\begin{tabular}{|c|c|c|c|c|c|@{\rule{0pt}{9pt}}}
\hline
& $\underline{P}(\mathcal V)$ & $\overline{P}(\mathcal V)$ & $\mathring{P}(\mathcal V)$ & $\max\{\mathfrak{f}_{\bfa{i}}\}$ & $\min\{\mathfrak{f}_\bfa{i}\}$\\
\hline HG $\abs{\bfa{i}}=16e4$ & $0.2909$ & $0.3138$ & $0.3002$ & $4.5e^{-5}$ & $3.0e^{-10}$ \\
\hline IG $\abs{\bfa{i}}=45436$ & $0.2954$ & $0.3045$ & $0.3025$ & $5.0e^{-5}$ & $4.4e^{-6}$ \\
\hline IG $\abs{\bfa{i}}=20404$ & $0.2871$ & $0.3050$ & $0.3008$ & $1.0e^{-4}$ & $1.3e^{-5}$ \\
\hline Sc $N=11$ & $0.2602$ & $0.7981$ & $0.5356$ & $5.0e^{-5}$ & $4.4e^{-6}$ \\ 
\hline Sc $N=26$ & $0.4834$ & $0.8875$ & $0.7375$ & $5.0e^{-5}$ & $4.4e^{-6}$ \\
\hline 
\end{tabular}
\captionsetup{font=small}
\caption{Probability bounds for homogeneous (HG) and non-homogeneous (IG) grids, and for scenario-based approaches (Sc).}
\label{tab:only:table}
%\vspace{1mm}
\end{table}




\section{Conclusion}\label{sec:conclusion}
%
%
%
%
%
\noindent In this paper we presented a method to optimise an auxiliary polytope guaranteeing constraint satisfaction of a probabilistic condition, as it could arise in an application of stochastic model predictive control.
%
To obtain a finite dimensional formulation of the optimisation, we assume a predetermined combinatorial structure of the auxiliary set, which allows for a finite parameterisation. 
%
The probabilistic measure of the auxiliary set is approximated using a Riemann-like approximation of the required integral, and by exploiting the fact that numerical integration over a hypercube is significantly simpler than over an arbitrary polytope.
%
We presented two methods of obtaining a suitable grid, a homogeneous grid can be generated simply and has an a priori known number of cubes, which could however be intractably large.
%
An inhomogeneous grid can be determined iteratively and will generally produce a smaller number of cubes, although the number of cubes is not a priori known.
%
For both methods the synthesis works identically and produced superior results over a conventional scenario-based approach in our example.
%


%
Similar to many other polytope-based methods the proposed scheme does not scale well with dimension $d$, this is due to the fact that the combinatorial structure itself becomes complex with rising dimension, higher dimensional problems will yield a large number of decision variables and nonlinear constraints.
%
Furthermore, the number of cubes ($Q_\bfa{i}$ or $Q_i$) that have to be evaluated during the optimisation becomes prohibitively large with increasing dimension for both homogeneous and inhomogeneous grids.
%
For both grids proposed here, the sample space was also assumed to be bounded. We note, however, that an inhomogeneous grid could be used in problems with unbounded sample spaces, for example by choosing a bounding box centred at the expected value with probability measure indistinguishable from one to machine precision.

\printbibliography
\end{document}